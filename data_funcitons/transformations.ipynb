{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document I collect all different kinds of transformations that I have coded up as funcitons for easy access and reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data\n",
    "- Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def fit_imputer(data):\n",
    "    '''\n",
    "    Fit a SimpleImputer to the data.\n",
    "    parameters:\n",
    "        data: pd.DataFrame\n",
    "            The data to fit the imputer on.\n",
    "    return: \n",
    "        SimpleImputer\n",
    "            The fitted imputer.\n",
    "    '''\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputer.fit(data)\n",
    "    imputer.statistics_\n",
    "    return imputer\n",
    "def transform_imputer(imputer, data):\n",
    "    '''\n",
    "    Transform the data using the fitted imputer.\n",
    "    parameters:\n",
    "        imputer: SimpleImputer\n",
    "            The fitted imputer.\n",
    "        data: pd.DataFrame\n",
    "            The data to transform.\n",
    "    return:\n",
    "        pd.DataFrame\n",
    "            The transformed data.\n",
    "    '''\n",
    "    return imputer.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "- Ordinal Encoding\n",
    "- One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding of Categorical data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def train_ordinal_encoder(data):\n",
    "    '''Train an ordinal encoder on the data'''\n",
    "    encoder = OrdinalEncoder()\n",
    "    encoded = encoder.fit_transform(data)\n",
    "    return encoder, encoded\n",
    "    \n",
    "def transform_ordinal_encoder(data, encoder):\n",
    "    '''Transform the data using the encoder'''\n",
    "    return encoder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of Categorical data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def train_1hot_encoder(data):\n",
    "    '''\n",
    "        The resulting encoder will encode new data based on the categories of the training data\n",
    "        ---\n",
    "        data: DataFrame \n",
    "            training DataFrame\n",
    "        return: \n",
    "            encoder\n",
    "                fitted encoder\n",
    "            encoded\n",
    "                DataFrame of encoded data\n",
    "    '''\n",
    "    encoder = OneHotEncoder()\n",
    "    sparce_matrix = encoder.fit_transform(data)\n",
    "    feature_names = encoder.categories_\n",
    "    encoded = sparce_matrix.toarray()\n",
    "    df_encoded = pd.DataFrame(encoded, columns=feature_names)\n",
    "    return encoder, df_encoded\n",
    "\n",
    "def fit_1hot_encoder(encoder, data):\n",
    "    '''\n",
    "        function to fit new data to a pre-trained encoder\n",
    "        ---\n",
    "        parameters:\n",
    "            encoder: OneHotEncoder\n",
    "                fitted encoder\n",
    "            data: DataFrame \n",
    "                DataFrame to be encoded\n",
    "        return: \n",
    "            df_encoded\n",
    "                DataFrame of encoded data\n",
    "    '''\n",
    "    sparce_matrix = encoder.transform(data)\n",
    "    feature_names = encoder.categories_\n",
    "    encoded = sparce_matrix.toarray()\n",
    "    df_encoded = pd.DataFrame(encoded, columns=feature_names)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "- min-max scaling\n",
    "- standard scaler\n",
    "- z score scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scaler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_minmax_scaler(data, min=0, max=1, clip=False):\n",
    "    '''Train a minmax scaler on the data'''\n",
    "    scaler = MinMaxScaler(feature_range=(min, max), clip=clip)\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    columns = scaler.feature_names_in_\n",
    "    df_scaled = pd.DataFrame(scaled, columns=columns)\n",
    "    return scaler, df_scaled\n",
    "\n",
    "def transform_minmax_scaler(data, scaler):\n",
    "    '''Transform the data using the scaler'''\n",
    "    scaled = scaler.transform(data)\n",
    "    columns = scaler.feature_names_in_\n",
    "    df_scaled = pd.DataFrame(scaled, columns=columns)\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_standard_scaler(data):\n",
    "    '''Train a standard scaler on the data'''\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    columns = scaler.feature_names_in_\n",
    "    df_scaled = pd.DataFrame(scaled, columns=columns)\n",
    "    return scaler, df_scaled\n",
    " \n",
    "def transform_standard_scaler(data, scaler):\n",
    "    '''Transform the data using the scaler'''\n",
    "    scaled = scaler.transform(data)\n",
    "    columns = scaler.feature_names_in_\n",
    "    df_scaled = pd.DataFrame(scaled, columns=columns)\n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score standardization implementation\n",
    "import numpy as np\n",
    "\n",
    "def z_score_standardization(col):\n",
    "    '''Normalizes a column using the z-score method to transform the data into units of standard deviations from the mean'''\n",
    "    col_mean = np.mean(col)\n",
    "    col_variance = np.std(col)\n",
    "    return((col-col_mean)/col_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "- log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def l1p(data):\n",
    "    '''\n",
    "    Log1p transformation of the data.\n",
    "    This transformation can be more beneficial than the standard log transformation \n",
    "    as it handles zero values.\n",
    "    ---\n",
    "    params: data: pd.Series\n",
    "    return: pd.Series\n",
    "    '''\n",
    "    assert data.min() < 0, 'data contains negative values at log1p transform'\n",
    "    return np.log1p(data)\n",
    "\n",
    "log1p_transformer = FunctionTransformer(func=l1p, validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering and plotting similarities in longitueds and latitudes\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=0.1, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f'Cluster {i} similarity' for i in range(self.n_clusters)]\n",
    "    \n",
    "# usage: either by explicit calls or in pipeline\n",
    "#   cluster_similarity = ClusterSimilarity(n_clusters=10, gamma=0.1, random_state=42)\n",
    "#   X = df_train_num[['latitude', 'longitude']]\n",
    "#   weight = df_train_num['median_house_value']\n",
    "#   ClusterSimilarity = cluster_similarity.fit(X, sample_weight=weight)\n",
    "#   similarities = cluster_similarity.transform(X)\n",
    "#   centroids = cluster_similarity.kmeans_.cluster_centers_\n",
    "\n",
    "# corresponding graph\n",
    "from matplotlib import pyplot as plt\n",
    "def plot_similarities(data, similarities, centroids):\n",
    "    plt.scatter(data[:0], data[:1], c=[np.max(c)for c in similarities], cmap='jet', s=20)\n",
    "    plt.scatter(centroids[:, 1], centroids[:, 0], c='black', marker='x', s=200)\n",
    "    plt.title('Cluster Centers and Clusters')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
